{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPS8X8ggWzR36HDLI5IwbAC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glitcher007/NLP/blob/main/NLP_preprocessing2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rb5htlUu_fgk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of breaking down a text into individual units, such as words or subwords, known as tokens. Tokenization is a crucial step in natural language processing (NLP) as it serves as the foundation for many downstream tasks.\n",
        "\n",
        "In Python, the Natural Language Toolkit (NLTK) and the spaCy library are popular choices for tokenization. I'll provide examples for both."
      ],
      "metadata": {
        "id": "MFk7_yF5_gEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('punkt')  # Download the punkt tokenizer (if not already downloaded)\n",
        "\n",
        "def tokenize_sentence(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    return sentences\n",
        "\n",
        "def tokenize_word(text):\n",
        "    words = word_tokenize(text)\n",
        "    return words\n",
        "\n",
        "# Example usage:\n",
        "text = \"Tokenization is an essential step in NLP. It breaks text into individual units, such as words or subwords.\"\n",
        "sentence_tokens = tokenize_sentence(text)\n",
        "word_tokens = tokenize_word(text)\n",
        "\n",
        "print(\"Sentence Tokens:\", sentence_tokens)\n",
        "print(\"\\nWord Tokens:\", word_tokens)\n"
      ],
      "metadata": {
        "id": "u2LEwNRy_gP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('punkt')  # Download the punkt tokenizer (if not already downloaded)\n",
        "\n",
        "def tokenize_sentence(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    return sentences\n",
        "\n",
        "def tokenize_word(text):\n",
        "    words = word_tokenize(text)\n",
        "    return words\n",
        "\n",
        "# Example usage:\n",
        "text = \"Tokenization is an essential step in NLP. It breaks text into individual units, such as words or subwords.\"\n",
        "sentence_tokens = tokenize_sentence(text)\n",
        "word_tokens = tokenize_word(text)\n",
        "\n",
        "print(\"Sentence Tokens:\", sentence_tokens)\n",
        "print(\"\\nWord Tokens:\", word_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvCnPAkm_lCO",
        "outputId": "5db27df7-04f9-43db-ab43-71e14fd68f03"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokens: ['Tokenization is an essential step in NLP.', 'It breaks text into individual units, such as words or subwords.']\n",
            "\n",
            "Word Tokens: ['Tokenization', 'is', 'an', 'essential', 'step', 'in', 'NLP', '.', 'It', 'breaks', 'text', 'into', 'individual', 'units', ',', 'such', 'as', 'words', 'or', 'subwords', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#spaCy Tokenization:"
      ],
      "metadata": {
        "id": "tH_EBZbX_sbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def spacy_tokenize(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc]\n",
        "    return tokens\n",
        "\n",
        "# Example usage:\n",
        "text = \"Tokenization is an essential step in NLP. It breaks text into individual units, such as words or subwords.\"\n",
        "spacy_tokens = spacy_tokenize(text)\n",
        "\n",
        "print(\"spaCy Tokens:\", spacy_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfzftrDc_mUE",
        "outputId": "b68e8cd8-914d-490f-b3d4-b8a1680c1089"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy Tokens: ['Tokenization', 'is', 'an', 'essential', 'step', 'in', 'NLP', '.', 'It', 'breaks', 'text', 'into', 'individual', 'units', ',', 'such', 'as', 'words', 'or', 'subwords', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#stemming"
      ],
      "metadata": {
        "id": "F-CU5VUY_ywL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming is a text normalization technique in natural language processing (NLP) that involves reducing words to their root or base form, known as the \"stem.\" The idea is to remove suffixes from words so that similar words map to the same stem.\n",
        "\n",
        "In Python, the Natural Language Toolkit (NLTK) provides a popular stemming module. One common stemming algorithm is the Porter Stemmer. Here's an example using NLTK:"
      ],
      "metadata": {
        "id": "ryN32yfG_wrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')  # Download the punkt tokenizer (if not already downloaded)\n",
        "\n",
        "def stem_text(text):\n",
        "    porter_stemmer = PorterStemmer()\n",
        "    words = word_tokenize(text)\n",
        "    stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "    stemmed_text = ' '.join(stemmed_words)\n",
        "    return stemmed_text\n",
        "\n",
        "# Example usage:\n",
        "text = \"Stemming is an essential technique for natural language processing. It helps to reduce words to their base form.\"\n",
        "stemmed_text = stem_text(text)\n",
        "print(\"Original Text:\", text)\n",
        "print(\"\\nStemmed Text:\", stemmed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhrSToWo_uLE",
        "outputId": "a7107596-c397-4873-a4cf-fdba7f4f6da9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Stemming is an essential technique for natural language processing. It helps to reduce words to their base form.\n",
            "\n",
            "Stemmed Text: stem is an essenti techniqu for natur languag process . it help to reduc word to their base form .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the PorterStemmer from NLTK is used to perform stemming on the input text. It's important to note that stemming may produce a stem that is not a valid word in the language. For example, \"running\" might be stemmed to \"run,\" but \"run\" is a valid word.\n",
        "\n",
        "While stemming can be beneficial for some tasks, it's not always suitable for all applications. In some cases, lemmatization (reducing words to their base or dictionary form) may be preferred, as it tends to produce valid words.\n",
        "\n",
        "Adjust the text normalization techniques based on your specific NLP task and the requirements of your text processing pipeline."
      ],
      "metadata": {
        "id": "RQCbfS6W_39W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install indic-nlp-library\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voXp0YwI_1Iu",
        "outputId": "ec6eadf0-1af7-4e00-9708-6d01c299b63c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.10/dist-packages (0.92)\n",
            "Requirement already satisfied: sphinx-argparse in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (0.4.0)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.23.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2023.3.post1)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (5.0.2)\n",
            "Requirement already satisfied: docutils<0.21 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (0.18.1)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->indic-nlp-library) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.7)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.9)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.6)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.2)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.14.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.13)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (23.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Hindi, you can use the Hindi Stemmer provided by the Indic NLP Library for Python. This library is specifically designed for processing Indian languages, including Hindi. The Indic NLP Library offers various linguistic processing tools, including stemmers, tokenizers, and part-of-speech taggers.\n",
        "\n",
        "Here's how you can use the Hindi Stemmer from the Indic NLP Library:"
      ],
      "metadata": {
        "id": "EkVNKn4QAEjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from indi-cnlp.stemmer import Stemmer\n",
        "\n",
        "def hindi_stem_text(text):\n",
        "    stemmer = Stemmer.Stemmer('hi')\n",
        "    stems = stemmer.stem(text)\n",
        "    stemmed_text = ' '.join(stems)\n",
        "    return stemmed_text\n",
        "\n",
        "# Example usage:\n",
        "text_hindi = \"हिन्दी भाषा में स्टेमिंग का उदाहारण।\"\n",
        "stemmed_text_hindi = hindi_stem_text(text_hindi)\n",
        "print(\"Original Text (Hindi):\", text_hindi)\n",
        "print(\"\\nStemmed Text (Hindi):\", stemmed_text_hindi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "6UrZ5ZDB_9Ae",
        "outputId": "4339174f-43af-4414-de73-b329117e3375"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-b89d9c3575b7>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from indi-cnlp.stemmer import Stemmer\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization is the process of reducing words to their base or root form, known as the \"lemma.\" Unlike stemming, which involves removing suffixes from words, lemmatization considers the entire word and transforms it into a valid word that exists in the language.\n",
        "\n",
        "In Python, the Natural Language Toolkit (NLTK) and spaCy are popular libraries that provide lemmatization functionality."
      ],
      "metadata": {
        "id": "7515V_s4AWj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')  # Download the punkt tokenizer (if not already downloaded)\n",
        "nltk.download('wordnet')  # Download the WordNet database (if not already downloaded)\n",
        "\n",
        "def lemmatize_text_nltk(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = word_tokenize(text)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    lemmatized_text = ' '.join(lemmatized_words)\n",
        "    return lemmatized_text\n",
        "\n",
        "# Example usage:\n",
        "text = \"Lemmatization is important for natural language processing. It helps to reduce words to their base form.\"\n",
        "lemmatized_text_nltk = lemmatize_text_nltk(text)\n",
        "print(\"Original Text:\", text)\n",
        "print(\"\\nLemmatized Text (NLTK):\", lemmatized_text_nltk)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSZ1k8LfAAiM",
        "outputId": "b0973973-bbec-44a2-e1d1-6dae7779a869"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Lemmatization is important for natural language processing. It helps to reduce words to their base form.\n",
            "\n",
            "Lemmatized Text (NLTK): Lemmatization is important for natural language processing . It help to reduce word to their base form .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def lemmatize_text_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "    return lemmatized_text\n",
        "\n",
        "# Example usage:\n",
        "text = \"Lemmatization is important for natural language processing. It helps to reduce words to their base form.\"\n",
        "lemmatized_text_spacy = lemmatize_text_spacy(text)\n",
        "print(\"Original Text:\", text)\n",
        "print(\"\\nLemmatized Text (spaCy):\", lemmatized_text_spacy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c52Q5D0gAYD9",
        "outputId": "b08417b9-e36d-40c8-dc88-1952e6bc8ec5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Lemmatization is important for natural language processing. It helps to reduce words to their base form.\n",
            "\n",
            "Lemmatized Text (spaCy): lemmatization be important for natural language processing . it help to reduce word to their base form .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jwFs6I-6AZtn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}