{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKm7qD4KJhCl+UTB+DHIet",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glitcher007/NLP/blob/main/NLP_pre_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To remove HTML tags from a string in NLP preprocessing, you can use various methods. One common approach is to use regular expressions. Here's a simple example in Python using the re module:"
      ],
      "metadata": {
        "id": "KXou6e4a91e8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCftd8yi9vne",
        "outputId": "e5b52715-ce50-4a66-d7f3-f81cf361579a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example HTML string.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    clean_text = re.sub('<.*?>', '', text)\n",
        "    return clean_text\n",
        "\n",
        "# Example usage:\n",
        "html_string = \"<p>This is an <b>example</b> HTML string.</p>\"\n",
        "cleaned_text = remove_html_tags(html_string)\n",
        "print(cleaned_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This remove_html_tags function uses the re.sub method to replace all occurrences of HTML tags with an empty string. The regular expression <.*?> matches any HTML tag and the .*? part ensures a non-greedy match, so it stops at the first closing angle bracket.\n",
        "\n",
        "Note that while this method can work for simple cases, it might not handle all edge cases in HTML parsing. For more robust HTML processing, you might want to consider using an HTML parser like BeautifulSoup:"
      ],
      "metadata": {
        "id": "iJ3Esr8s97DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def remove_html_tags_bs4(text):\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    clean_text = soup.get_text()\n",
        "    return clean_text\n",
        "\n",
        "# Example usage:\n",
        "html_string = \"<p>This is an <b>example</b> HTML string.</p>\"\n",
        "cleaned_text = remove_html_tags_bs4(html_string)\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n39oONN794aq",
        "outputId": "90046755-431a-463e-af06-5fdd2c15df78"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example HTML string.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, BeautifulSoup is used to parse the HTML, and get_text() is used to extract the text content without HTML tags. This method is generally more robust and handles complex HTML structures better."
      ],
      "metadata": {
        "id": "TPVT4NPi-Aw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_urls(text):\n",
        "    # This regular expression matches URLs\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub('', text)\n",
        "\n",
        "# Example usage:\n",
        "text_with_urls = \"Check out this website: https://www.example.com for more information.\"\n",
        "text_without_urls = remove_urls(text_with_urls)\n",
        "print(text_without_urls)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "808IiyJI99l7",
        "outputId": "c866d272-0114-46f2-e1c2-4559ad95b16e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check out this website:  for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the remove_urls function uses the re.sub method to replace URLs with an empty string. The regular expression https?://\\S+|www\\.\\S+ is designed to match both HTTP and HTTPS URLs, as well as URLs starting with \"www.\"\n",
        "\n",
        "Keep in mind that removing URLs in this way might not handle all edge cases, and it's a relatively simple approach. If your text contains complex URL variations or if you need a more sophisticated solution, you might want to explore using specialized URL parsing libraries or additional pre-processing techniques."
      ],
      "metadata": {
        "id": "PjxrnTG0-Fqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # Create a translation table with None for each punctuation character\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # Use translate to remove punctuation\n",
        "    text_without_punct = text.translate(translator)\n",
        "    return text_without_punct\n",
        "\n",
        "# Example usage:\n",
        "text_with_punct = \"Hello, World! This is an example text with some punctuation.\"\n",
        "text_without_punct = remove_punctuation(text_with_punct)\n",
        "print(text_without_punct)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u17XhRfE-DHG",
        "outputId": "bab942a4-7752-4fa2-968f-a416188a3459"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World This is an example text with some punctuation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_punctuation_regex(text):\n",
        "    # Use regex to replace all punctuation characters with an empty string\n",
        "    text_without_punct = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text_without_punct\n",
        "\n",
        "# Example usage:\n",
        "text_with_punct = \"Hello, World! This is an example text with some punctuation.\"\n",
        "text_without_punct = remove_punctuation_regex(text_with_punct)\n",
        "print(text_without_punct)\n"
      ],
      "metadata": {
        "id": "-qI0ZynG-IQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both methods will remove common punctuation marks from the input string, leaving only the alphanumeric characters and spaces. Choose the method that best fits your needs or based on your preference for readability and simplicity."
      ],
      "metadata": {
        "id": "Msgci-c--McH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Spelling correction in NLP involves identifying and correcting misspelled words in a text. One popular library for spelling correction in Python is textblob. Here's a simple example:"
      ],
      "metadata": {
        "id": "dBwWhYIX-UkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def correct_spelling(text):\n",
        "    blob = TextBlob(text)\n",
        "    corrected_text = str(blob.correct())\n",
        "    return corrected_text\n",
        "\n",
        "# Example usage:\n",
        "text_with_spelling_errors = \"Ths is an example of a sentnce with sme speling mistake.\"\n",
        "corrected_text = correct_spelling(text_with_spelling_errors)\n",
        "print(corrected_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2nwycye-M7q",
        "outputId": "6b177ced-0fc0-4e84-a052-5ba61824f758"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The is an example of a sentence with she spelling mistake.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "np_UiENy-WcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "n this example, the TextBlob library is used to create a text blob object, and the correct() method is applied to correct the spelling. Note that this is a basic example, and the accuracy of spelling correction may vary based on the context and the specific library or method used.\n",
        "\n",
        "Keep in mind that there are other more sophisticated spelling correction models and libraries available, such as the symspellpy library and various language-specific models. These may provide better performance for specific use cases or domains.\n",
        "\n",
        "Here's an example using the symspellpy library"
      ],
      "metadata": {
        "id": "PBKP_yNB-YuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "xTTgKtp7-cAR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Removing stop words is a common preprocessing step in NLP to filter out common words that do not contribute much to the meaning of a sentence. In Python, you can use the Natural Language Toolkit (nltk) library to remove stop words. Here's an example:"
      ],
      "metadata": {
        "id": "zqC399zg-mYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_text)\n",
        "\n",
        "# Example usage:\n",
        "text_with_stop_words = \"This is an example sentence with some common stop words.\"\n",
        "text_without_stop_words = remove_stop_words(text_with_stop_words)\n",
        "print(text_without_stop_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp36e6KW-eu4",
        "outputId": "4c7c7577-1831-4586-d8e1-951b15515b1c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example sentence common stop words .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the stopwords.words('english') function from nltk is used to obtain a set of English stop words. The word_tokenize function is then used to tokenize the input text into words. Finally, a list comprehension is employed to filter out stop words, and the result is joined back into a string.\n",
        "\n",
        "Keep in mind that the list of stop words can vary based on the language and the specific requirements of your task. If you're working with a language other than English, you can replace 'english' with the appropriate language code (e.g., 'spanish', 'french').\n",
        "\n",
        "Adjust the code based on your specific needs, and consider the requirements of your NLP task when determining whether or not to remove stop words."
      ],
      "metadata": {
        "id": "Ct-sKerx-scT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To remove emojis from a text in NLP preprocessing, you can use a regular expression. Emojis are often represented as Unicode characters, and you can match and remove them using regex. Here's an example in Python:"
      ],
      "metadata": {
        "id": "u3dJZysw-vIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YJ5AQjFf-vBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_emojis(text):\n",
        "    # Emoji regex pattern\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
        "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "\n",
        "    # Remove emojis\n",
        "    text_without_emojis = emoji_pattern.sub(r'', text)\n",
        "    return text_without_emojis\n",
        "\n",
        "# Example usage:\n",
        "text_with_emojis = \"Hello! üòä How are you today? üåç\"\n",
        "text_without_emojis = remove_emojis(text_with_emojis)\n",
        "print(text_without_emojis)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdvF0dFX-qEm",
        "outputId": "b13ce3d5-ab46-4808-d8ba-dd16f0936ea3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello!  How are you today? \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a remove_emojis function that uses a regular expression to match and remove emojis from the input text. Adjust the regex pattern as needed for your specific use case.\n",
        "\n",
        "Keep in mind that emojis can convey valuable information, and removing them may not be suitable for all NLP tasks, particularly those involving sentiment analysis or emotion detection. Customize the code based on your specific requirements."
      ],
      "metadata": {
        "id": "MIjjcnYr-0D8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems there might be a slight error in your question, and I assume you're asking about \"lemmatization\" rather than \"demonization.\" Lemmatization is a text normalization process in NLP that involves reducing words to their base or root form. It helps in grouping together different inflected forms of a word.\n",
        "\n",
        "For lemmatization in Python, you can use the NLTK library, which provides a WordNetLemmatizer class. Here's an example:"
      ],
      "metadata": {
        "id": "9u2zR7d7-2o0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_pos(word)) for word in tokens]\n",
        "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "    return lemmatized_text\n",
        "\n",
        "def get_pos(word):\n",
        "    # Get the part of speech for WordNet lemmatization\n",
        "    pos_tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    pos_mapping = {'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV, 'J': wordnet.ADJ}\n",
        "    return pos_mapping.get(pos_tag, wordnet.NOUN)\n",
        "\n",
        "# Example usage:\n",
        "text_to_lemmatize = \"The cats are running\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu_m-pBW-xPG",
        "outputId": "f5953e13-5d03-4dcb-a740-688b0e0cdc8e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aC4NYPTw_FkA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears you are referring to the process of \"demojization\" in the context of NLP. Demojization typically involves converting emojis into their textual representations. In Python, you can use the emoji library to perform demojization. If you don't have the library installed, you can install it using:"
      ],
      "metadata": {
        "id": "fWsjiyG3_FQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ewv9zBu6-6jW",
        "outputId": "35c29b87-ee23-4394-b31f-c3c3808214c1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "\n",
        "def demojize_text(text):\n",
        "    demojized_text = emoji.demojize(text)\n",
        "    return demojized_text\n",
        "\n",
        "# Example usage:\n",
        "text_with_emojis = \"Hello! üòä How are you today? üåç\"\n",
        "demojized_text = demojize_text(text_with_emojis)\n",
        "print(demojized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxmQh6zK_G6w",
        "outputId": "95599673-2e15-46ed-af98-8916b66eeefe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! :smiling_face_with_smiling_eyes: How are you today? :globe_showing_Europe-Africa:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the emoji.demojize() function is applied to convert emojis into their textual representations. For instance, \"üòä\" might be converted to \":smiling_face_with_smiling_eyes:\".\n",
        "\n",
        "Keep in mind that demojization might be useful in certain NLP tasks where you want to analyze or process text without considering the emojis. Adjust the code based on your specific needs and the requirements of your NLP task."
      ],
      "metadata": {
        "id": "JcmHOVza_LOf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "29YpYi9G_Jhs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}